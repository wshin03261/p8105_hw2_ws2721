---
title: "p8105_hw2_ws2721"
author: "WooJin Shin"
output: github_document
date: "2024-09-30"
---

Problem 1
------------------------------------------------------------
NYC train

```{r echo = FALSE, message = FALSE}
library(dplyr)
library(tidyr)
library(tidyverse)
library(readxl)
```

```{r}
nyc = read.csv("./data/NYC_Transit_Subway_Entrance_And_Exit_data.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, 
         contains("route"), entry, vending, entrance_type, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

```{r eval = FALSE}
summary(nyc)
```

```{r eval = FALSE}
nrow(nyc)
```

```{r eval = FALSE}
ncol(nyc)
```

First, I imported the data and converted the variable names to snake cases.
Then, I have selected the variables that I only need and changed the entry variable
to a logical variable.
After the data process, there are 19 columns and 1868 rows.
The data contains 19 variables consists with:
11 character variables (routes 1-7,line, station name, etc.),6 numerical
variables (station latitude, longitude, etc.) and 2 logical variables (entry, ada).

```{r}
nyc2 = nyc |> 
  mutate(route8 = as.character(route8), route9 = as.character(route9), 
         route10 = as.character(route10), route11 = as.character(route11)) |>
  pivot_longer(cols = route1:route11, names_to = "route", names_prefix = "route",
               values_to = "sub_line") |> 
  drop_na("sub_line")
```

After looking at the questions, to see the distinct stations clearly, I merged the routes to be attached in one station that will make easier for us to isolate the routes
Route 8 through 11 were converted to characters to match the format with route 1~7.
Also removed the NA to have only lanes that are attached to the station. This will make the dataset more tidy.

```{r eval = FALSE}
station = distinct(nyc2, line, station_name)

nrow(station)
```

There are 465 distinct stations identified by both name and line.

```{r eval = FALSE}
compliant = distinct(nyc2, line, station_name, ada) |> 
  filter(ada == TRUE)

nrow(compliant)
```

Among 465 distinct stations, 84 are ADA compliant.

```{r eval = FALSE}
allow_entry = filter(nyc2, vending == "NO")|>
  summarise(mean(entry == TRUE))

allow_entry
```

The proportion of station entrances / exits without vending allow entrance is 0.311.

```{r eval = FALSE}
distinct2 = distinct(nyc2, station_name, ada, sub_line) |> 
  filter(sub_line == "A")

nrow(distinct2)
```

57 distinct stations serve A train.

```{r eval = FALSE}
distinct3 = distinct(nyc2, station_name, ada, sub_line) |> 
  filter(sub_line == "A", ada == TRUE)

nrow(distinct3)
```

Out of 57 distinct stations in A train, 16 are ADA compliant.


Problem 2
------------------------------------------------------------
Mr. Trash Wheel

```{r}

mr_trash = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                   range = "A2:N653", sheet = "Mr. Trash Wheel") |> 
  janitor::clean_names() |> 
  mutate(sports_balls = as.integer(round(sports_balls)),
         year = as.double(year),
         indicator = "Mr. Trash")

prof_trash = read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                         range = "A2:M121", sheet = "Professor Trash Wheel") |>
  janitor::clean_names() |> 
  mutate(indicator = "Prof. Trash")

gwynnda_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      range = "A2:L265", sheet = "Gwynnda Trash Wheel") |>
  janitor::clean_names() |> 
  mutate(indicator = "Gwynnda")

trash_df = bind_rows(mr_trash, prof_trash, gwynnda_trash)
```

I have added the indicator variable for all 3 data sets that I am aggregating and changed the type of variable for Mr. Trash wheel because it gives an error saying that you cannot bind the chr and dbl variables together. So I had to change it to dbl before I bind.

The merged data frame consists of the time, the type and amount of trash gathered in the designated time frame among 3 trash collecting robots. The date is divided to month and year as well as the type of trash (plastic bottles, cigarette_butts .etc) and the amount (weight_tons, volume_cubic_yards .etc). The total amounts of home powered are also included in the data frame.The indicator variable separates the robot in the data frame. 

```{r}
total_prof = trash_df |> 
  filter(indicator == "Prof. Trash") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))
```

Total collected from Professor Trash Wheel is 246.74 tons.

```{r}
total_cig_gwynnda = trash_df |> 
  filter(indicator == "Gwynnda", month == "June", year == "2022") |> 
  summarize(total_cig = sum(cigarette_butts, na.rm = TRUE))
```

Total cigarette butts collected by Gwynnda in June of 2022 is 18120.


Problem 3
------------------------------------------------------------
Great British Bake Off

```{r}
bakers = read.csv("./data/gbb_datasets/bakers.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |> 
  arrange(series, baker)

bakes = read.csv("./data/gbb_datasets/bakes.csv",
                 na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  arrange(series, baker)

results = read.csv("./data/gbb_datasets/results.csv", skip = 2,
                 na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  arrange(series, baker)
```

Bakers:
Looking at the data, we first have to separate the baker name to 2 parts, their first name and last name because bakes and results already have their first name.
Then arrange the table to concentrate on series and baker for future binding.

Bakes: 
Looking at the data, nothing typical was shown. Arrange the table by their series and baker for future binding.

Results:
Looking at the data, we have to omit the first 2 rows to clear the data. Then, arrange the table by their series and baker for future binding.

```{r}
anti_join(bakers, bakes)
```

```{r}
anti_join(bakers, results)
```

```{r}
anti_join(bakes, results)
```

Before merging, we have to check the data. I used `anti_join` to assess the data. It identifies the observations that exist in the first data set but not in the second data set.

Comparing bakers, bakes:
It joined by baker and series. In series 2, "Jo Wheatley" wasn't in the bakers list and bakers from series 9 and 10 weren't as well.

Comparing bakers, results:
It joined by baker and series. I again, see the same name "Jo Wheatley" but this time both data set have series 9 and 10.

Comparing bakes, results:
It joined by baker, series, and episode. Again, I see "Jo" as well.

Since all 3 commands with `anti_join` shows "Jo" from series 2 that tells us it is worth looking into.

By looking at the result data: 
"Joanne" from the result data looks like it matches with "Jo" in both bakers and bakes data. I will fix this name to Jo before making a data frame.

```{r}
bakes = bakes |> 
  mutate(baker = gsub('"Jo"','Jo', baker))
```

```{r}
results = results |> 
  mutate(baker = gsub('Joanne','Jo', baker))
```

I've changed Joanne and "Jo" from series 2 to Jo.

Then we will bind the data set to make it reader friendly.

```{r}
bakers_df = left_join(results, bakers, by = c("series", "baker")) |>
  left_join(bakes, by = c("series", "episode", "baker")) |>
  select(series, episode, baker, result, everything()) |> 
  drop_na()

write_csv(bakers_df, "./data/gbb_datasets/bakers_df.csv")
```

Merged the data and exported

```{r}
stars_or_winner = bakers_df |>
  filter(series >= 5 & series <= 10 & 
           (result == "STAR BAKER" | result == "WINNER"))
```

There are total of 40 titles of "Winner" and "Star Baker" awarded to 20 distinct bakers in between series 5 and 10. I could see that the most of the winners have technical level that is around 1 to 2 of the winning bake.
Also, generally people who won the STAR BAKER prior seems like to have a higher chance of winning in the series.

```{r}
viewers = read.csv("./data/gbb_datasets/viewers.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(cols = series_1:series_10, names_to = "series", 
               names_prefix = "series_", values_to = "viewer") |>
  filter(!is.na(viewer)) |>
    mutate(series = as.double(series)) |>
  arrange(series, episode)
```

Made the data longer that shows the list of episodes, series, and the viewers that are associated with. The series were chraracter variable so I had to change it to double in order to filter for future average calculation.

```{r}
head(viewers, n = 10)
```

```{r}
mean1 = viewers |>
  filter(series == 1) |>
  summarise(mean = mean(viewer)) |>
  pull(mean)

mean5 = viewers |>
  filter(series == 5) |>
  summarise(mean = mean(viewer)) |>
  pull(mean)
```

The average viewership of season 1 was 2.77 and season 5 was 10.04.
