p8105_hw2_ws2721
================
WooJin Shin
2024-09-30

## Problem 1

NYC train

``` r
nyc = read.csv("./data/NYC_Transit_Subway_Entrance_And_Exit_data.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, 
         contains("route"), entry, vending, entrance_type, ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

``` r
summary(nyc)
```

``` r
nrow(nyc)
```

``` r
ncol(nyc)
```

First, I imported the data and converted the variable names to snake
cases. Then, I have selected the variables that I only need and changed
the entry variable to a logical variable. After the data process, there
are 19 columns and 1868 rows. The data contains 19 variables consists
with: 11 character variables (routes 1-7,line, station name, etc.),6
numerical variables (station latitude, longitude, etc.) and 2 logical
variables (entry, ada).

``` r
nyc2 = nyc |> 
  mutate(route8 = as.character(route8), route9 = as.character(route9), 
         route10 = as.character(route10), route11 = as.character(route11)) |>
  pivot_longer(cols = route1:route11, names_to = "route", names_prefix = "route",
               values_to = "sub_line") |> 
  drop_na("sub_line")
```

After looking at the questions, to see the distinct stations clearly, I
merged the routes to be attached in one station that will make easier
for us to isolate the routes Route 8 through 11 were converted to
characters to match the format with route 1~7. Also removed the NA to
have only lanes that are attached to the station. This will make the
dataset more tidy.

``` r
station = distinct(nyc2, line, station_name)

nrow(station)
```

There are 465 distinct stations identified by both name and line.

``` r
compliant = distinct(nyc2, line, station_name, ada) |> 
  filter(ada == TRUE)

nrow(compliant)
```

Among 465 distinct stations, 84 are ADA compliant.

``` r
allow_entry = filter(nyc2, vending == "NO")|>
  summarise(mean(entry == TRUE))

allow_entry
```

The proportion of station entrances / exits without vending allow
entrance is 0.311.

``` r
distinct2 = distinct(nyc2, station_name, ada, sub_line) |> 
  filter(sub_line == "A")

nrow(distinct2)
```

57 distinct stations serve A train.

``` r
distinct3 = distinct(nyc2, station_name, ada, sub_line) |> 
  filter(sub_line == "A", ada == TRUE)

nrow(distinct3)
```

Out of 57 distinct stations in A train, 16 are ADA compliant.

## Problem 2

Mr. Trash Wheel

``` r
mr_trash = read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                   range = "A2:N653", sheet = "Mr. Trash Wheel") |> 
  janitor::clean_names() |> 
  mutate(sports_balls = as.integer(round(sports_balls)),
         year = as.double(year),
         indicator = "Mr. Trash")

prof_trash = read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                         range = "A2:M121", sheet = "Professor Trash Wheel") |>
  janitor::clean_names() |> 
  mutate(indicator = "Prof. Trash")

gwynnda_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      range = "A2:L265", sheet = "Gwynnda Trash Wheel") |>
  janitor::clean_names() |> 
  mutate(indicator = "Gwynnda")

trash_df = bind_rows(mr_trash, prof_trash, gwynnda_trash)
```

I have added the indicator variable for all 3 data sets that I am
aggregating and changed the type of variable for Mr. Trash wheel because
it gives an error saying that you cannot bind the chr and dbl variables
together. So I had to change it to dbl before I bind.

The merged data frame consists of the time, the type and amount of trash
gathered in the designated time frame among 3 trash collecting robots.
The date is divided to month and year as well as the type of trash
(plastic bottles, cigarette_butts .etc) and the amount (weight_tons,
volume_cubic_yards .etc). The total amounts of home powered are also
included in the data frame.The indicator variable separates the robot in
the data frame.

``` r
total_prof = trash_df |> 
  filter(indicator == "Prof. Trash") |> 
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))
```

Total collected from Professor Trash Wheel is 246.74 tons.

``` r
total_cig_gwynnda = trash_df |> 
  filter(indicator == "Gwynnda", month == "June", year == "2022") |> 
  summarize(total_cig = sum(cigarette_butts, na.rm = TRUE))
```

Total cigarette butts collected by Gwynnda in June of 2022 is 18120.

## Problem 3

Great British Bake Off

``` r
bakers = read.csv("./data/gbb_datasets/bakers.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |> 
  arrange(series, baker)

bakes = read.csv("./data/gbb_datasets/bakes.csv",
                 na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  arrange(series, baker)

results = read.csv("./data/gbb_datasets/results.csv", skip = 2,
                 na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  arrange(series, baker)
```

Bakers: Looking at the data, we first have to separate the baker name to
2 parts, their first name and last name because bakes and results
already have their first name. Then arrange the table to concentrate on
series and baker for future binding.

Bakes: Looking at the data, nothing typical was shown. Arrange the table
by their series and baker for future binding.

Results: Looking at the data, we have to omit the first 2 rows to clear
the data. Then, arrange the table by their series and baker for future
binding.

``` r
anti_join(bakers, bakes)
```

    ## Joining with `by = join_by(baker, series)`

    ##       baker         last_name series baker_age
    ## 1        Jo          Wheatley      2        41
    ## 2    Antony         Amourdoux      9        30
    ## 3    Briony          Williams      9        33
    ## 4       Dan   Beasley-Harling      9        36
    ## 5    Imelda          McCarron      9        33
    ## 6       Jon           Jenkins      9        47
    ## 7     Karen            Wright      9        60
    ## 8   Kim-Joy           Hewlett      9        27
    ## 9      Luke          Thompson      9        30
    ## 10    Manon           Lagrave      9        26
    ## 11    Rahul            Mandal      9        30
    ## 12     Ruby            Bhogal      9        29
    ## 13    Terry           Hartill      9        56
    ## 14    Alice          Fevronia     10        28
    ## 15   Amelia           LeBruin     10        24
    ## 16      Dan          Chambers     10        32
    ## 17    David          Atherton     10        36
    ## 18   Helena            Garcia     10        40
    ## 19    Henry              Bird     10        20
    ## 20    Jamie              Finn     10        20
    ## 21  Michael       Chakraverty     10        26
    ## 22 Michelle       Evans-Fecci     10        35
    ## 23     Phil            Thorne     10        56
    ## 24    Priya            O'Shea     10        34
    ## 25    Rosie Brandreth-Poynter     10        28
    ## 26    Steph         Blackwell     10        28
    ##                      baker_occupation            hometown
    ## 1                           Housewife        Ongar, Essex
    ## 2                              Banker              London
    ## 3                    Full-time parent             Bristol
    ## 4                    Full-time parent              London
    ## 5      Countryside recreation officer       County Tyrone
    ## 6                       Blood courier             Newport
    ## 7         In-store sampling assistant           Wakefield
    ## 8            Mental health specialist               Leeds
    ## 9   Civil servant/house and techno DJ           Sheffield
    ## 10           Software project manager              London
    ## 11                 Research scientist           Rotherham
    ## 12                    Project manager              London
    ## 13                Retired air steward       West Midlands
    ## 14                  Geography teacher               Essex
    ## 15                   Fashion designer             Halifax
    ## 16                     Support worker           Rotherham
    ## 17       International health adviser              Whitby
    ## 18             Online project manager               Leeds
    ## 19                            Student              Durham
    ## 20                   Part-time waiter              Surrey
    ## 21 Theatre manager/fitness instructor Stratford-upon-Avon
    ## 22           Print shop administrator        Tenby, Wales
    ## 23                         HGV driver             Rainham
    ## 24               Marketing consultant           Leicester
    ## 25                 Veterinary surgeon            Somerset
    ## 26                     Shop assistant             Chester

``` r
anti_join(bakers, results)
```

    ## Joining with `by = join_by(baker, series)`

    ##   baker last_name series baker_age baker_occupation     hometown
    ## 1    Jo  Wheatley      2        41        Housewife Ongar, Essex

``` r
anti_join(bakes, results)
```

    ## Joining with `by = join_by(series, episode, baker)`

    ##   series episode baker
    ## 1      2       1  "Jo"
    ## 2      2       2  "Jo"
    ## 3      2       3  "Jo"
    ## 4      2       4  "Jo"
    ## 5      2       5  "Jo"
    ## 6      2       6  "Jo"
    ## 7      2       7  "Jo"
    ## 8      2       8  "Jo"
    ##                                                signature_bake
    ## 1       Chocolate Orange CupcakesOrange and Cardamom Cupcakes
    ## 2                 Caramelised Onion, Gruyere and Thyme Quiche
    ## 3 Stromboli flavored with Mozzarella, Ham, and Picante Pepper
    ## 4                                           Lavender Biscuits
    ## 5                                    Salmon and Asparagus Pie
    ## 6                             Rum and Raisin Baked Cheesecake
    ## 7                          Raspberry & Strawberry Mousse Cake
    ## 8                       Raspberry and Blueberry Mille Feuille
    ##                                                                                                          show_stopper
    ## 1                                                                                       Chocolate and Strawberry Cake
    ## 2                   Raspberry and Mascarpone Tarts with Lemon and Almond PastryHoney and Almond Tarts with Sweetcrust
    ## 3                                                                                                             Unknown
    ## 4                                                               Blueberry MacaronsCoconut MacaronsStrawberry Macarons
    ## 5                                                                                    Apple and Raspberry Meringue Pie
    ## 6                                                                        Limoncello and White Chocolate Croquembouche
    ## 7                                                           Pain Aux RaisinChocolate TwistsBanana and Raisin Pastries
    ## 8 Mini Victoria SandwichesRaspberry, White Chocolate and Pistachio Mini MeringuesMini Banoffee Pie with Banana Mousse

Before merging, we have to check the data. I used `anti_join` to assess
the data. It identifies the observations that exist in the first data
set but not in the second data set.

Comparing bakers, bakes: It joined by baker and series. In series 2, “Jo
Wheatley” wasn’t in the bakers list and bakers from series 9 and 10
weren’t as well.

Comparing bakers, results: It joined by baker and series. I again, see
the same name “Jo Wheatley” but this time both data set have series 9
and 10.

Comparing bakes, results: It joined by baker, series, and episode.
Again, I see “Jo” as well.

Since all 3 commands with `anti_join` shows “Jo” from series 2 that
tells us it is worth looking into.

By looking at the result data: “Joanne” from the result data looks like
it matches with “Jo” in both bakers and bakes data. I will fix this name
to Jo before making a data frame.

``` r
bakes = bakes |> 
  mutate(baker = gsub('"Jo"','Jo', baker))
```

``` r
results = results |> 
  mutate(baker = gsub('Joanne','Jo', baker))
```

I’ve changed Joanne and “Jo” from series 2 to Jo.

Then we will bind the data set to make it reader friendly.

``` r
bakers_df = left_join(results, bakers, by = c("series", "baker")) |>
  left_join(bakes, by = c("series", "episode", "baker")) |>
  select(series, episode, baker, result, everything()) |> 
  arrange(series, episode) |> 
  drop_na(result)

write_csv(bakers_df, "./data/gbb_datasets/bakers_df.csv")
```

Merged the data and exported I only dropped the missing values for the
result to organize because if you drop all the NAs in the data set,
there are some rows that get dropped that you shouldn’t drop. There are
some values of NA in technical among people that were still in the
competition.

``` r
stars_or_winner = bakers_df |>
  filter(series >= 5 & series <= 10 & 
           (result == "STAR BAKER" | result == "WINNER"))
```

There are total of 60 titles of “Winner” and “Star Baker” awarded to 32
distinct bakers in between series 5 and 10. I could see that the most of
the winners have technical level that is around 1 to 2 of the winning
bake. Also, generally people who won the STAR BAKER prior seems like to
have a higher chance of winning in the series. There is a star baker or
a winner in every episode of the series.

``` r
viewers = read.csv("./data/gbb_datasets/viewers.csv", 
               na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(cols = series_1:series_10, names_to = "series", 
               names_prefix = "series_", values_to = "viewer") |>
  filter(!is.na(viewer)) |>
    mutate(series = as.double(series)) |>
  arrange(series, episode)
```

Made the data longer that shows the list of episodes, series, and the
viewers that are associated with. The series were chraracter variable so
I had to change it to double in order to filter for future average
calculation.

``` r
head(viewers, n = 10)
```

    ## # A tibble: 10 × 3
    ##    episode series viewer
    ##      <int>  <dbl>  <dbl>
    ##  1       1      1   2.24
    ##  2       2      1   3   
    ##  3       3      1   3   
    ##  4       4      1   2.6 
    ##  5       5      1   3.03
    ##  6       6      1   2.75
    ##  7       1      2   3.1 
    ##  8       2      2   3.53
    ##  9       3      2   3.82
    ## 10       4      2   3.6

``` r
mean1 = viewers |>
  filter(series == 1) |>
  summarise(mean = mean(viewer)) |>
  pull(mean)

mean5 = viewers |>
  filter(series == 5) |>
  summarise(mean = mean(viewer)) |>
  pull(mean)
```

The average viewership of season 1 was 2.77 and season 5 was 10.04.
